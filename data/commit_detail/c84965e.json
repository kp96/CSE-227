{"sha":"\"47e7bc67f932866e4467f7ab8a23bb167405bebd\"","commit":{"author":{"name":"\"Nicolas Silva\"","email":"\"nsilva@mozilla.com\"","date":"\"2016-09-01T15:51:09+02:00\""},"committer":{"name":"\"Nicolas Silva\"","email":"\"nsilva@mozilla.com\"","date":"\"2016-09-01T15:51:09+02:00\""},"message":"\"Bug 1298345 - Refactor CanvasRenderingContext2D's texture allocation code. r=Basc84965e\""},"diff":"\"c84965e Bug 392263: using mmap/VirtualAlloc for GC arenas. r=brendan\\ndiff --git a/js/src/jsarray.c b/js/src/jsarray.c\\nindex aa22f63..8d0feaf 100644\\n--- a/js/src/jsarray.c\\n+++ b/js/src/jsarray.c\\n@@ -2111,6 +2111,6 @@ js_NewArrayObject(JSContext *cx, jsuint length, jsval *vector)\\n     JS_POP_TEMP_ROOT(cx, &tvr);\\n \\n     /* Set/clear newborn root, in case we lost it.  */\\n-    cx->weakRoots.newborn[GCX_OBJECT] = (JSGCThing *) obj;\\n+    cx->weakRoots.newborn[GCX_OBJECT] = obj;\\n     return obj;\\n }\\ndiff --git a/js/src/jscntxt.h b/js/src/jscntxt.h\\nindex edf9f87..fa3a47c 100644\\n--- a/js/src/jscntxt.h\\n+++ b/js/src/jscntxt.h\\n@@ -171,6 +171,7 @@ struct JSRuntime {\\n     JSContextCallback   cxCallback;\\n \\n     /* Garbage collector state, used by jsgc.c. */\\n+    JSGCChunkInfo       *gcChunkList;\\n     JSGCArenaList       gcArenaList[GC_NUM_FREELISTS];\\n     JSDHashTable        gcRootsHash;\\n     JSDHashTable        *gcLocksHash;\\n@@ -202,9 +203,9 @@ struct JSRuntime {\\n     JSGCThingCallback   gcThingCallback;\\n     void                *gcThingCallbackClosure;\\n     uint32              gcMallocBytes;\\n-    JSGCArena           *gcUnscannedArenaStackTop;\\n+    JSGCArenaInfo       *gcUntracedArenaStackTop;\\n #ifdef DEBUG\\n-    size_t              gcUnscannedBagSize;\\n+    size_t              gcTraceLaterCount;\\n #endif\\n \\n     /*\\ndiff --git a/js/src/jsfun.c b/js/src/jsfun.c\\nindex 5901d41..9bcdc91 100644\\n--- a/js/src/jsfun.c\\n+++ b/js/src/jsfun.c\\n@@ -1173,8 +1173,7 @@ fun_resolve(JSContext *cx, JSObject *obj, jsval id, uintN flags,\\n                  * root until then to protect pval in case it is figuratively\\n                  * up in the air, with no strong refs protecting it.\\n                  */\\n-                cx->weakRoots.newborn[GCX_OBJECT] =\\n-                    (JSGCThing *)JSVAL_TO_GCTHING(pval);\\n+                cx->weakRoots.newborn[GCX_OBJECT] = JSVAL_TO_GCTHING(pval);\\n                 parentProto = JSVAL_TO_OBJECT(pval);\\n             }\\n         }\\ndiff --git a/js/src/jsgc.c b/js/src/jsgc.c\\nindex 67a5bea..56d4751 100644\\n--- a/js/src/jsgc.c\\n+++ b/js/src/jsgc.c\\n@@ -79,151 +79,269 @@\\n #endif\\n \\n /*\\n- * GC arena sizing depends on amortizing arena overhead using a large number\\n- * of things per arena, and on the thing/flags ratio of 8:1 on most platforms.\\n- *\\n- * On 64-bit platforms, we would have half as many things per arena because\\n- * pointers are twice as big, so we double the bytes for things per arena.\\n- * This preserves the 1024 byte flags sub-arena size, which relates to the\\n- * GC_PAGE_SIZE (see below for why).\\n+ * Deduce if mmap or similar is available.\\n  */\\n-#if JS_BYTES_PER_WORD == 8\\n-# define GC_THINGS_SHIFT 14     /* 16KB for things on Alpha, etc. */\\n-#else\\n-# define GC_THINGS_SHIFT 13     /* 8KB for things on most platforms */\\n+#ifndef JS_GC_USE_MMAP\\n+# if defined(XP_WIN)\\n+#  define JS_GC_USE_MMAP 1\\n+# elif defined(XP_UNIX) || defined(XP_BEOS)\\n+#  include <unistd.h>\\n+#  if defined(_POSIX_MAPPED_FILES) && _POSIX_MAPPED_FILES > 0\\n+#   define JS_GC_USE_MMAP 1\\n+#  endif\\n+# endif\\n+#endif\\n+\\n+#ifndef JS_GC_USE_MMAP\\n+# define JS_GC_USE_MMAP 0\\n+#endif\\n+\\n+#if JS_GC_USE_MMAP\\n+# if defined(XP_WIN)\\n+#  include <windows.h>\\n+# elif defined(XP_UNIX) || defined(XP_BEOS)\\n+#  include <sys/mman.h>\\n+\\n+/* On Mac OS X MAP_ANONYMOUS is not defined. */\\n+#  if !defined(MAP_ANONYMOUS) && defined(MAP_ANON)\\n+#   define MAP_ANONYMOUS MAP_ANON\\n+#  endif\\n+\\n+# endif\\n #endif\\n-#define GC_THINGS_SIZE  JS_BIT(GC_THINGS_SHIFT)\\n-#define GC_FLAGS_SIZE   (GC_THINGS_SIZE / sizeof(JSGCThing))\\n \\n /*\\n  * A GC arena contains one flag byte for each thing in its heap, and supports\\n  * O(1) lookup of a flag given its thing's address.\\n  *\\n- * To implement this, we take advantage of the thing/flags numerology: given\\n- * the 8K bytes worth of GC-things, there are 1K flag bytes. Within each 9K\\n- * allocation for things+flags there are always 8 consecutive 1K-pages each\\n- * aligned on 1K boundary. We use these pages to allocate things and the\\n- * remaining 1K of space before and after the aligned pages to store flags.\\n- * If we are really lucky and things+flags starts on a 1K boundary, then\\n- * flags would consist of a single 1K chunk that comes after 8K of things.\\n- * Otherwise there are 2 chunks of flags, one before and one after things.\\n+ * To implement this, we allocate things of the same size from a GC arena\\n+ * containing GC_ARENA_SIZE bytes aligned on GC_ARENA_SIZE boundary. The\\n+ * following picture shows arena's layout:\\n  *\\n- * To be able to find the flag byte for a particular thing, we put a\\n- * JSGCPageInfo record at the beginning of each 1K-aligned page to hold that\\n- * page's offset from the beginning of things+flags allocation and we allocate\\n- * things after this record. Thus for each thing |thing_address & ~1023|\\n- * gives the address of a JSGCPageInfo record from which we read page_offset.\\n- * Due to page alignment\\n- *  (page_offset & ~1023) + (thing_address & 1023)\\n- * gives thing_offset from the beginning of 8K paged things. We then divide\\n- * thing_offset by sizeof(JSGCThing) to get thing_index.\\n+ *  +------------------------------+--------------------+---------------+\\n+ *  | allocation area for GC thing | flags of GC things | JSGCArenaInfo |\\n+ *  +------------------------------+--------------------+---------------+\\n  *\\n- * Now |page_address - page_offset| is things+flags arena_address and\\n- * (page_offset & 1023) is the offset of the first page from the start of\\n- * things+flags area. Thus if\\n- *  thing_index < (page_offset & 1023)\\n- * then\\n- *  allocation_start_address + thing_index < address_of_the_first_page\\n- * and we use\\n- *  allocation_start_address + thing_index\\n- * as the address to store thing's flags. If\\n- *  thing_index >= (page_offset & 1023),\\n- * then we use the chunk of flags that comes after the pages with things\\n- * and calculate the address for the flag byte as\\n- *  address_of_the_first_page + 8K + (thing_index - (page_offset & 1023))\\n- * which is just\\n- *  allocation_start_address + thing_index + 8K.\\n+ * For a GC thing of size thingSize the number of things that the arena can\\n+ * hold is given by:\\n+ *   (GC_ARENA_SIZE - sizeof(JSGCArenaInfo)) / (thingSize + 1)\\n  *\\n- * When we allocate things with size equal to sizeof(JSGCThing), the overhead\\n- * of this scheme for 32 bit platforms is (8+8*(8+1))/(8+9K) or 0.87%\\n- * (assuming 4 bytes for each JSGCArena header, and 8 bytes for each\\n- * JSGCThing and JSGCPageInfo). When thing_size > 8, the scheme wastes the\\n- * flag byte for each extra 8 bytes beyond sizeof(JSGCThing) in thing_size\\n- * and the overhead is close to 1/8 or 12.5%.\\n- * FIXME: How can we avoid this overhead?\\n+ * The address of thing's flag is given by:\\n+ *   flagByteAddress =\\n+ *       (thingAddress | GC_ARENA_MASK) - sizeof(JSGCArenaInfo) -\\n+ *       (thingAddress & GC_ARENA_MASK) / thingSize\\n+ * where\\n+ *   (thingAddress | GC_ARENA_MASK) - sizeof(JSGCArenaInfo)\\n+ * is the last byte of flags' area and\\n+ *   (thingAddress & GC_ARENA_MASK) / thingSize\\n+ * is thing's index counting from arena's start.\\n  *\\n- * Here's some ASCII art showing an arena:\\n+ * Things are allocated from the start of their area and flags are allocated\\n+ * from the end of their area. This avoids calculating the location of the\\n+ * boundary separating things and flags.\\n  *\\n- *   split or the first 1-K aligned address.\\n- *     |\\n- *     V\\n- *  +--+-------+-------+-------+-------+-------+-------+-------+-------+-----+\\n- *  |fB|  tp0  |  tp1  |  tp2  |  tp3  |  tp4  |  tp5  |  tp6  |  tp7  | fA  |\\n- *  +--+-------+-------+-------+-------+-------+-------+-------+-------+-----+\\n- *              ^                                 ^\\n- *  tI ---------+                                 |\\n- *  tJ -------------------------------------------+\\n+ * JS_GC_USE_MMAP macros governs the allocation of aligned arenas. When the\\n+ * macro is true, a platform specific allocation code like POSIX mmap is used\\n+ * with no extra overhead. If the macro is false, the code uses malloc to\\n+ * allocate a chunk of\\n+ *   GC_ARENA_SIZE * (js_gcArenasPerChunk + 1)\\n+ * bytes. The chunk contains at least js_gcArenasPerChunk aligned arenas so\\n+ * the overhead of this schema is approximately 1/js_gcArenasPerChunk. See\\n+ * NewGCChunk/DestroyGCChunk below for details.\\n  *\\n- *  - fB are the \\\"before split\\\" flags, fA are the \\\"after split\\\" flags\\n- *  - tp0-tp7 are the 8 thing pages\\n- *  - thing tI points into tp1, whose flags are below the split, in fB\\n- *  - thing tJ points into tp5, clearly above the split\\n- *\\n- * In general, one of the thing pages will have some of its things' flags on\\n- * the low side of the split, and the rest of its things' flags on the high\\n- * side.  All the other pages have flags only below or only above.\\n+ * Note that even when JS_GC_USE_MMAP is true the code still allocates arenas\\n+ * in chunks to minimize the overhead of mmap/munmap.\\n+ */\\n+\\n+/*\\n+ * When mmap is available, use the minimal known CPU page size as the size of\\n+ * GC arena. Otherwise use 1K arenas to minimize the overhead of the aligned\\n+ * allocation.\\n+ */\\n+#if JS_GC_USE_MMAP\\n+# define GC_ARENA_SHIFT              12\\n+#else\\n+# define GC_ARENA_SHIFT              10\\n+#endif\\n+\\n+#define GC_ARENA_MASK               ((jsuword) JS_BITMASK(GC_ARENA_SHIFT))\\n+#define GC_ARENA_SIZE               JS_BIT(GC_ARENA_SHIFT)\\n+\\n+struct JSGCArenaInfo {\\n+    /*\\n+     * Allocation list for the arena.\\n+     */\\n+    JSGCArenaList   *list;\\n+\\n+    /*\\n+     * Pointer to the previous arena in a linked list. The arena can either\\n+     * belong to one of JSContext.gcArenaList lists or, when it does not have\\n+     * any allocated GC things, to the list of free arenas in the chunk with\\n+     * head stored in JSGCChunkInfo.lastFreeArena.\\n+     */\\n+    JSGCArenaInfo   *prev;\\n+\\n+    /*\\n+     * A link field for the list of arenas with marked but not yet traced\\n+     * things. The field is encoded as arena's page to share the space with\\n+     * firstArena and arenaIndex fields.\\n+     */\\n+    jsuword         prevUntracedPage :  JS_BITS_PER_WORD - GC_ARENA_SHIFT;\\n+\\n+    /*\\n+     * When firstArena is false, the index of arena in the chunk. When\\n+     * firstArena is true, the index of a free arena holding JSGCChunkInfo or\\n+     * NO_FREE_ARENAS if there are no free arenas in the chunk.\\n+     *\\n+     * GET_ARENA_INDEX and GET_CHUNK_INFO_INDEX are convenience macros to\\n+     * access either of indexes.\\n+     */\\n+    jsuword         arenaIndex :        GC_ARENA_SHIFT - 1;\\n+\\n+    /*\\n+     * Flag indicating if the arena is the first in the chunk.\\n+     */\\n+    jsuword         firstArena :        1;\\n+\\n+    /*\\n+     * Bitset for fast search of marked but not yet traced things.\\n+     */\\n+    jsuword         untracedThings;\\n+};\\n+\\n+/*\\n+ * Verify that the bit fields are indeed shared and JSGCArenaInfo is as small\\n+ * as possible. The code does not rely on this check so if on a particular\\n+ * platform this does not compile, then, as a workaround, comment the assert\\n+ * out and submit a bug report.\\n+ */\\n+JS_STATIC_ASSERT(sizeof(JSGCArenaInfo) == 4 * sizeof(jsuword));\\n+\\n+#define NO_FREE_ARENAS              JS_BITMASK(GC_ARENA_SHIFT - 1)\\n+\\n+/*\\n+ * All chunks that have at least one free arena are put on the doubly-linked\\n+ * list with the head stored in JSRuntime.gcChunkList. JSGCChunkInfo contains\\n+ * the head of the chunk's free arena list together with the link fields for\\n+ * gcChunkList.\\n  *\\n- * (If we need to implement card-marking for an incremental GC write barrier,\\n- * we can replace word-sized offsetInArena in JSGCPageInfo by pair of\\n- * uint8 card_mark and uint16 offsetInArena fields as the offset can not exceed\\n- * GC_THINGS_SIZE. This would gives an extremely efficient write barrier:\\n- * when mutating an object obj, just store a 1 byte at\\n- * (uint8 *) ((jsuword)obj & ~1023) on 32-bit platforms.)\\n+ * The structure is stored in one of chunk's free arenas. GET_CHUNK_INFO_INDEX\\n+ * macro gives the index of this arena. When all arenas in the chunk are used,\\n+ * it is removed from the list and the index is set to NO_FREE_ARENAS\\n+ * indicating that the chunk is not on gcChunkList and has no JSGCChunkInfo\\n+ * available.\\n  */\\n-#define GC_PAGE_SHIFT   10\\n-#define GC_PAGE_MASK    ((jsuword) JS_BITMASK(GC_PAGE_SHIFT))\\n-#define GC_PAGE_SIZE    JS_BIT(GC_PAGE_SHIFT)\\n-#define GC_PAGE_COUNT   (1 << (GC_THINGS_SHIFT - GC_PAGE_SHIFT))\\n-\\n-typedef struct JSGCPageInfo {\\n-    jsuword     offsetInArena;          /* offset from the arena start */\\n-    jsuword     unscannedBitmap;        /* bitset for fast search of marked\\n-                                           but not yet scanned GC things */\\n-} JSGCPageInfo;\\n-\\n-struct JSGCArena {\\n-    JSGCArenaList       *list;          /* allocation list for the arena */\\n-    JSGCArena           *prev;          /* link field for allocation list */\\n-    JSGCArena           *prevUnscanned; /* link field for the list of arenas\\n-                                           with marked but not yet scanned\\n-                                           things */\\n-    jsuword             unscannedPages; /* bitset for fast search of pages\\n-                                           with marked but not yet scanned\\n-                                           things */\\n-    uint8               base[1];        /* things+flags allocation area */\\n+struct JSGCChunkInfo {\\n+    JSGCChunkInfo   **prevp;\\n+    JSGCChunkInfo   *next;\\n+    JSGCArenaInfo   *lastFreeArena;\\n+    uint32          numFreeArenas;\\n };\\n \\n-#define GC_ARENA_SIZE                                                         \\\\\\n-    (offsetof(JSGCArena, base) + GC_THINGS_SIZE + GC_FLAGS_SIZE)\\n+/*\\n+ * Even when mmap is available, its overhead may be too big so the final\\n+ * decision to use it is done at runtime.\\n+ */\\n+#if JS_GC_USE_MMAP\\n+static uint32 js_gcArenasPerChunk = 0;\\n+static JSBool js_gcUseMmap = JS_FALSE;\\n+#else\\n+# define js_gcArenasPerChunk 31\\n+#endif\\n+\\n+/*\\n+ * Macros to convert between JSGCArenaInfo, the start address of the arena and\\n+ * arena's page defined as (start address) >> GC_ARENA_SHIFT.\\n+ */\\n+#define ARENA_INFO_OFFSET (GC_ARENA_SIZE - (uint32) sizeof(JSGCArenaInfo))\\n \\n-#define FIRST_THING_PAGE(a)                                                   \\\\\\n-    (((jsuword)(a)->base + GC_FLAGS_SIZE - 1) & ~GC_PAGE_MASK)\\n+#define IS_ARENA_INFO_ADDRESS(arena)                                          \\\\\\n+    (((jsuword) (arena) & GC_ARENA_MASK) == ARENA_INFO_OFFSET)\\n \\n-#define PAGE_TO_ARENA(pi)                                                     \\\\\\n-    ((JSGCArena *)((jsuword)(pi) - (pi)->offsetInArena                        \\\\\\n-                   - offsetof(JSGCArena, base)))\\n+#define ARENA_START_TO_INFO(arenaStart)                                       \\\\\\n+    (JS_ASSERT(((arenaStart) & (jsuword) GC_ARENA_MASK) == 0),                \\\\\\n+     (JSGCArenaInfo *) ((arenaStart) + ARENA_INFO_OFFSET))\\n \\n-#define PAGE_INDEX(pi)                                                        \\\\\\n-    ((size_t)((pi)->offsetInArena >> GC_PAGE_SHIFT))\\n+#define ARENA_INFO_TO_START(arena)                                            \\\\\\n+    (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arena)),                                 \\\\\\n+     (jsuword) (arena) & ~(jsuword) GC_ARENA_MASK)\\n \\n-#define THING_TO_PAGE(thing)                                                  \\\\\\n-    ((JSGCPageInfo *)((jsuword)(thing) & ~GC_PAGE_MASK))\\n+#define ARENA_PAGE_TO_INFO(arenaPage)                                         \\\\\\n+    (JS_ASSERT(arenaPage != 0),                                               \\\\\\n+     JS_ASSERT(((arenaPage) >> (JS_BITS_PER_WORD - GC_ARENA_SHIFT)) == 0),    \\\\\\n+     ARENA_START_TO_INFO((arenaPage) << GC_ARENA_SHIFT))\\n+\\n+#define ARENA_INFO_TO_PAGE(arena)                                             \\\\\\n+    (JS_ASSERT(IS_ARENA_INFO_ADDRESS(arena)),                                 \\\\\\n+     ((jsuword) (arena) >> GC_ARENA_SHIFT))\\n+\\n+#define GET_ARENA_INFO(chunk, index)                                          \\\\\\n+    (JS_ASSERT((index) < js_gcArenasPerChunk),                                \\\\\\n+     ARENA_START_TO_INFO(chunk + ((index) << GC_ARENA_SHIFT)))\\n \\n /*\\n- * Given a thing size n, return the size of the gap from the page start before\\n- * the first thing.  We know that any n not a power of two packs from\\n- * the end of the page leaving at least enough room for one JSGCPageInfo, but\\n- * not for another thing, at the front of the page (JS_ASSERTs below insist\\n- * on this).\\n- *\\n- * This works because all allocations are a multiple of sizeof(JSGCThing) ==\\n- * sizeof(JSGCPageInfo) in size.\\n+ * Macros to access/modify information about the chunk of GC arenas.\\n+ */\\n+#define GET_ARENA_CHUNK(arena, index)                                         \\\\\\n+    (JS_ASSERT(GET_ARENA_INDEX(arena) == index),                              \\\\\\n+     ARENA_INFO_TO_START(arena) - ((index) << GC_ARENA_SHIFT))\\n+\\n+#define GET_ARENA_INDEX(arena)                                                \\\\\\n+    ((arena)->firstArena ? 0 : (uint32) (arena)->arenaIndex)\\n+\\n+#define GET_CHUNK_INFO_INDEX(chunk)                                           \\\\\\n+    ((uint32) ARENA_START_TO_INFO(chunk)->arenaIndex)\\n+\\n+#define SET_CHUNK_INFO_INDEX(chunk, index)                                    \\\\\\n+    (JS_ASSERT((index) < js_gcArenasPerChunk || (index) == NO_FREE_ARENAS),   \\\\\\n+     (void) (ARENA_START_TO_INFO(chunk)->arenaIndex = (jsuword) (index)))\\n+\\n+#define GET_CHUNK_INFO(chunk, infoIndex)                                      \\\\\\n+    (JS_ASSERT(GET_CHUNK_INFO_INDEX(chunk) == (infoIndex)),                   \\\\\\n+     JS_ASSERT((uint32) (infoIndex) < js_gcArenasPerChunk),                   \\\\\\n+     (JSGCChunkInfo *) ((chunk) + ((infoIndex) << GC_ARENA_SHIFT)))\\n+\\n+#define CHUNK_INFO_TO_INDEX(ci)                                               \\\\\\n+    GET_ARENA_INDEX(ARENA_START_TO_INFO((jsuword)ci))\\n+\\n+/*\\n+ * Macros for GC-thing operations.\\n  */\\n-#define PAGE_THING_GAP(n) (((n) & ((n) - 1)) ? (GC_PAGE_SIZE % (n)) : (n))\\n+#define THINGS_PER_ARENA(thingSize)                                           \\\\\\n+    ((GC_ARENA_SIZE - (uint32) sizeof(JSGCArenaInfo)) / ((thingSize) + 1U))\\n+\\n+#define THING_TO_ARENA(thing)                                                 \\\\\\n+    ((JSGCArenaInfo *)(((jsuword) (thing) | GC_ARENA_MASK) +                  \\\\\\n+                       1 - sizeof(JSGCArenaInfo)))\\n+\\n+#define THING_TO_INDEX(thing, thingSize)                                      \\\\\\n+    ((uint32) ((jsuword) (thing) & GC_ARENA_MASK) / (uint32) (thingSize))\\n+\\n+#define THING_FLAGS_END(arena) ((uint8 *)(arena))\\n+\\n+#define THING_FLAGP(arena, thingIndex)                                        \\\\\\n+    (JS_ASSERT((jsuword) (thingIndex)                                         \\\\\\n+               < (jsuword) THINGS_PER_ARENA((arena)->list->thingSize)),       \\\\\\n+     (uint8 *)(arena) - 1 - (thingIndex))\\n+\\n+#define THING_TO_FLAGP(thing, thingSize)                                      \\\\\\n+    THING_FLAGP(THING_TO_ARENA(thing), THING_TO_INDEX(thing, thingSize))\\n+\\n+#define FLAGP_TO_ARENA(flagp) THING_TO_ARENA(flagp)\\n+\\n+#define FLAGP_TO_INDEX(flagp)                                                 \\\\\\n+    (JS_ASSERT(((jsuword) (flagp) & GC_ARENA_MASK) < ARENA_INFO_OFFSET),      \\\\\\n+     (ARENA_INFO_OFFSET - 1 - (uint32) ((jsuword) (flagp) & GC_ARENA_MASK)))\\n+\\n+#define FLAGP_TO_THING(flagp, thingSize)                                      \\\\\\n+    (JS_ASSERT(((jsuword) (flagp) & GC_ARENA_MASK) >=                         \\\\\\n+               (ARENA_INFO_OFFSET - THINGS_PER_ARENA(thingSize))),            \\\\\\n+     (void *)(((jsuword) (flagp) & ~GC_ARENA_MASK) +                          \\\\\\n+              (thingSize) * FLAGP_TO_INDEX(flagp)))\\n \\n #ifdef JS_THREADSAFE\\n /*\\n- * The maximum number of things to put to the local free list by taking\\n+ * The maximum number of things to put on the local free list by taking\\n  * several things from the global free list or from the tail of the last\\n  * allocated arena to amortize the cost of rt->gcLock.\\n  *\\n@@ -233,8 +351,6 @@ struct JSGCArena {\\n \\n #endif\\n \\n-JS_STATIC_ASSERT(sizeof(JSGCThing) == sizeof(JSGCPageInfo));\\n-JS_STATIC_ASSERT(GC_FLAGS_SIZE >= GC_PAGE_SIZE);\\n JS_STATIC_ASSERT(sizeof(JSStackHeader) >= 2 * sizeof(jsval));\\n \\n JS_STATIC_ASSERT(sizeof(JSGCThing) >= sizeof(JSString));\\n@@ -414,63 +530,217 @@ ShrinkPtrTable(JSPtrTable *table, const JSPtrTableInfo *info,\\n # define METER(x) ((void) 0)\\n #endif\\n \\n-static JSBool\\n-NewGCArena(JSRuntime *rt, JSGCArenaList *arenaList)\\n+/*\\n+ * For chunks allocated via over-sized malloc, get a pointer to store the gap\\n+ * between the malloc's result and the first arena in the chunk.\\n+ */\\n+static uint32 *\\n+GetMallocedChunkGapPtr(jsuword chunk)\\n {\\n-    JSGCArena *a;\\n-    jsuword offset;\\n-    JSGCPageInfo *pi;\\n+    JS_ASSERT((chunk & GC_ARENA_MASK) == 0);\\n \\n-    /* Check if we are allowed and can allocate a new arena. */\\n-    if (rt->gcBytes >= rt->gcMaxBytes)\\n-        return JS_FALSE;\\n-    a = (JSGCArena *)malloc(GC_ARENA_SIZE);\\n-    if (!a)\\n-        return JS_FALSE;\\n+    /* Use the memory after the chunk, see NewGCChunk for details. */\\n+    return (uint32 *) (chunk + (js_gcArenasPerChunk << GC_ARENA_SHIFT));\\n+}\\n \\n-    /* Initialize the JSGCPageInfo records at the start of every thing page. */\\n-    offset = (GC_PAGE_SIZE - ((jsuword)a->base & GC_PAGE_MASK)) & GC_PAGE_MASK;\\n-    JS_ASSERT((jsuword)a->base + offset == FIRST_THING_PAGE(a));\\n-    do {\\n-        pi = (JSGCPageInfo *) (a->base + offset);\\n-        pi->offsetInArena = offset;\\n-        pi->unscannedBitmap = 0;\\n-        offset += GC_PAGE_SIZE;\\n-    } while (offset < GC_THINGS_SIZE);\\n-\\n-    METER(++arenaList->stats.narenas);\\n-    METER(arenaList->stats.maxarenas\\n-          = JS_MAX(arenaList->stats.maxarenas, arenaList->stats.narenas));\\n-\\n-    a->list = arenaList;\\n-    a->prev = arenaList->last;\\n-    a->prevUnscanned = NULL;\\n-    a->unscannedPages = 0;\\n-    arenaList->last = a;\\n-    arenaList->lastLimit = 0;\\n-    rt->gcBytes += GC_ARENA_SIZE;\\n-    return JS_TRUE;\\n+static jsuword\\n+NewGCChunk()\\n+{\\n+    void *p;\\n+    jsuword chunk;\\n+\\n+#if JS_GC_USE_MMAP\\n+    if (js_gcUseMmap) {\\n+# if defined(XP_WIN)\\n+        p = VirtualAlloc(NULL, js_gcArenasPerChunk << GC_ARENA_SHIFT,\\n+                         MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE);\\n+        return (jsuword) p;\\n+# elif defined(XP_UNIX) || defined(XP_BEOS)\\n+        p = mmap(NULL, js_gcArenasPerChunk << GC_ARENA_SHIFT,\\n+                 PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\\n+        return (p == MAP_FAILED) ? 0 : (jsuword) p;\\n+# else\\n+#  error \\\"Not implemented\\\"\\n+# endif\\n+    }\\n+#endif\\n+\\n+    /*\\n+     * Implement the chunk allocation using over sized malloc if mmap cannot\\n+     * be used. FIXME bug 396007: the code should use posix_memalign when it\\n+     * is available.\\n+     *\\n+     * Since malloc allocates pointers aligned on the word boundary, to get\\n+     * js_gcArenasPerChunk aligned arenas, we need to malloc only\\n+     *   ((js_gcArenasPerChunk + 1) << GC_ARENA_SHIFT) - sizeof(size_t)\\n+     * bytes. But since we stores the gap between the malloced pointer and the\\n+     * first arena in the chunk after the chunk, we need to ask for\\n+     *   ((js_gcArenasPerChunk + 1) << GC_ARENA_SHIFT)\\n+     * bytes to ensure that we always have room to store the gap.\\n+     */\\n+    p = malloc((js_gcArenasPerChunk + 1) << GC_ARENA_SHIFT);\\n+    if (!p)\\n+        return 0;\\n+    chunk = ((jsuword) p + GC_ARENA_MASK) & ~GC_ARENA_MASK;\\n+    *GetMallocedChunkGapPtr(chunk) = (uint32) (chunk - (jsuword) p);\\n+    return chunk;\\n }\\n \\n static void\\n-DestroyGCArena(JSRuntime *rt, JSGCArenaList *arenaList, JSGCArena **ap)\\n+DestroyGCChunk(jsuword chunk)\\n {\\n-    JSGCArena *a;\\n+    JS_ASSERT((chunk & GC_ARENA_MASK) == 0);\\n+#if JS_GC_USE_MMAP\\n+    if (js_gcUseMmap) {\\n+# if defined(XP_WIN)\\n+        VirtualFree((void *) chunk, 0, MEM_RELEASE);\\n+# elif defined(XP_UNIX) || defined(XP_BEOS)\\n+        munmap((void *) chunk, js_gcArenasPerChunk << GC_ARENA_SHIFT);\\n+# else\\n+#  error \\\"Not implemented\\\"\\n+# endif\\n+        return;\\n+    }\\n+#endif\\n+\\n+    /* See comments in NewGCChunk. */\\n+    JS_ASSERT(*GetMallocedChunkGapPtr(chunk) < GC_ARENA_SIZE);\\n+    free((void *) (chunk - *GetMallocedChunkGapPtr(chunk)));\\n+}\\n+\\n+static void\\n+AddChunkToList(JSRuntime *rt, JSGCChunkInfo *ci)\\n+{\\n+    ci->prevp = &rt->gcChunkList;\\n+    ci->next = rt->gcChunkList;\\n+    if (rt->gcChunkList) {\\n+        JS_ASSERT(rt->gcChunkList->prevp == &rt->gcChunkList);\\n+        rt->gcChunkList->prevp = &ci->next;\\n+    }\\n+    rt->gcChunkList = ci;\\n+}\\n+\\n+static void\\n+RemoveChunkFromList(JSRuntime *rt, JSGCChunkInfo *ci)\\n+{\\n+    *ci->prevp = ci->next;\\n+    if (ci->next) {\\n+        JS_ASSERT(ci->next->prevp == &ci->next);\\n+        ci->next->prevp = ci->prevp;\\n+    }\\n+}\\n+\\n+static JSGCArenaInfo *\\n+NewGCArena(JSRuntime *rt)\\n+{\\n+    jsuword chunk;\\n+    JSGCChunkInfo *ci;\\n+    uint32 i;\\n+    JSGCArenaInfo *a, *aprev;\\n+\\n+    if (js_gcArenasPerChunk == 1) {\\n+        chunk = NewGCChunk();\\n+        return (chunk == 0) ? NULL : ARENA_START_TO_INFO(chunk);\\n+    }\\n+\\n+    ci = rt->gcChunkList;\\n+    if (!ci) {\\n+        chunk = NewGCChunk();\\n+        if (chunk == 0)\\n+            return NULL;\\n+        JS_ASSERT((chunk & GC_ARENA_MASK) == 0);\\n+        a = GET_ARENA_INFO(chunk, 0);\\n+        a->firstArena = JS_TRUE;\\n+        a->arenaIndex = 0;\\n+        aprev = NULL;\\n+        i = 0;\\n+        do {\\n+            a->prev = aprev;\\n+            aprev = a;\\n+            ++i;\\n+            a = GET_ARENA_INFO(chunk, i);\\n+            a->firstArena = JS_FALSE;\\n+            a->arenaIndex = i;\\n+        } while (i != js_gcArenasPerChunk - 1);\\n+        ci = GET_CHUNK_INFO(chunk, 0);\\n+        ci->lastFreeArena = aprev;\\n+        ci->numFreeArenas = js_gcArenasPerChunk - 1;\\n+        AddChunkToList(rt, ci);\\n+    } else {\\n+        JS_ASSERT(ci->prevp == &rt->gcChunkList);\\n+        a = ci->lastFreeArena;\\n+        aprev = a->prev;\\n+        if (!aprev) {\\n+            JS_ASSERT(ci->numFreeArenas == 1);\\n+            JS_ASSERT(ARENA_INFO_TO_START(a) == (jsuword) ci);\\n+            RemoveChunkFromList(rt, ci);\\n+            chunk = GET_ARENA_CHUNK(a, GET_ARENA_INDEX(a));\\n+            SET_CHUNK_INFO_INDEX(chunk, NO_FREE_ARENAS);\\n+        } else {\\n+            JS_ASSERT(ci->numFreeArenas >= 2);\\n+            JS_ASSERT(ARENA_INFO_TO_START(a) != (jsuword) ci);\\n+            ci->lastFreeArena = aprev;\\n+            ci->numFreeArenas--;\\n+        }\\n+    }\\n+\\n+    return a;\\n+}\\n+\\n+static void\\n+DestroyGCArena(JSRuntime *rt, JSGCArenaInfo *a)\\n+{\\n+    uint32 arenaIndex;\\n+    jsuword chunk;\\n+    uint32 chunkInfoIndex;\\n+    JSGCChunkInfo *ci;\\n \\n-    a = *ap;\\n-    JS_ASSERT(a);\\n-    JS_ASSERT(rt->gcBytes >= GC_ARENA_SIZE);\\n-    rt->gcBytes -= GC_ARENA_SIZE;\\n     METER(rt->gcStats.afree++);\\n-    METER(--arenaList->stats.narenas);\\n-    if (a == arenaList->last)\\n-        arenaList->lastLimit = (uint16)(a->prev ? GC_THINGS_SIZE : 0);\\n-    *ap = a->prev;\\n+\\n+    if (js_gcArenasPerChunk == 1) {\\n+        DestroyGCChunk(ARENA_INFO_TO_START(a));\\n+        return;\\n+    }\\n \\n #ifdef DEBUG\\n-    memset(a, JS_FREE_PATTERN, GC_ARENA_SIZE);\\n+    {\\n+        jsuword firstArena, arenaIndex;\\n+\\n+        firstArena = a->firstArena;\\n+        arenaIndex = a->arenaIndex;\\n+        memset((void *) ARENA_INFO_TO_START(a), JS_FREE_PATTERN,\\n+               GC_ARENA_SIZE);\\n+        a->firstArena = firstArena;\\n+        a->arenaIndex = arenaIndex;\\n+    }\\n #endif\\n-    free(a);\\n+\\n+    arenaIndex = GET_ARENA_INDEX(a);\\n+    chunk = GET_ARENA_CHUNK(a, arenaIndex);\\n+    chunkInfoIndex = GET_CHUNK_INFO_INDEX(chunk);\\n+    if (chunkInfoIndex == NO_FREE_ARENAS) {\\n+        chunkInfoIndex = arenaIndex;\\n+        SET_CHUNK_INFO_INDEX(chunk, arenaIndex);\\n+        ci = GET_CHUNK_INFO(chunk, chunkInfoIndex);\\n+        a->prev = NULL;\\n+        ci->lastFreeArena = a;\\n+        ci->numFreeArenas = 1;\\n+        AddChunkToList(rt, ci);\\n+    } else {\\n+        JS_ASSERT(chunkInfoIndex != arenaIndex);\\n+        ci = GET_CHUNK_INFO(chunk, chunkInfoIndex);\\n+        JS_ASSERT(ci->numFreeArenas != 0);\\n+        JS_ASSERT(ci->lastFreeArena);\\n+        JS_ASSERT(a != ci->lastFreeArena);\\n+        if (ci->numFreeArenas == js_gcArenasPerChunk - 1) {\\n+            RemoveChunkFromList(rt, ci);\\n+            DestroyGCChunk(chunk);\\n+        } else {\\n+            ++ci->numFreeArenas;\\n+            a->prev = ci->lastFreeArena;\\n+            ci->lastFreeArena = a;\\n+        }\\n+    }\\n }\\n \\n static void\\n@@ -484,7 +754,7 @@ InitGCArenaLists(JSRuntime *rt)\\n         thingSize = GC_FREELIST_NBYTES(i);\\n         JS_ASSERT((size_t)(uint16)thingSize == thingSize);\\n         arenaList->last = NULL;\\n-        arenaList->lastLimit = 0;\\n+        arenaList->lastCount = THINGS_PER_ARENA(thingSize);\\n         arenaList->thingSize = (uint16)thingSize;\\n         arenaList->freeList = NULL;\\n         METER(memset(&arenaList->stats, 0, sizeof arenaList->stats));\\n@@ -496,40 +766,41 @@ FinishGCArenaLists(JSRuntime *rt)\\n {\\n     uintN i;\\n     JSGCArenaList *arenaList;\\n+    JSGCArenaInfo *a, *aprev;\\n \\n     for (i = 0; i < GC_NUM_FREELISTS; i++) {\\n         arenaList = &rt->gcArenaList[i];\\n-        while (arenaList->last)\\n-            DestroyGCArena(rt, arenaList, &arenaList->last);\\n+\\n+        for (a = arenaList->last; a; a = aprev) {\\n+            aprev = a->prev;\\n+            DestroyGCArena(rt, a);\\n+        }\\n+        arenaList->last = NULL;\\n+        arenaList->lastCount = THINGS_PER_ARENA(arenaList->thingSize);\\n         arenaList->freeList = NULL;\\n+        METER(arenaList->stats.narenas = 0);\\n     }\\n+    rt->gcBytes = 0;\\n+    JS_ASSERT(rt->gcChunkList == 0);\\n }\\n \\n JS_FRIEND_API(uint8 *)\\n js_GetGCThingFlags(void *thing)\\n {\\n-    JSGCPageInfo *pi;\\n-    jsuword offsetInArena, thingIndex;\\n-\\n-    pi = THING_TO_PAGE(thing);\\n-    offsetInArena = pi->offsetInArena;\\n-    JS_ASSERT(offsetInArena < GC_THINGS_SIZE);\\n-    thingIndex = ((offsetInArena & ~GC_PAGE_MASK) |\\n-                  ((jsuword)thing & GC_PAGE_MASK)) / sizeof(JSGCThing);\\n-    JS_ASSERT(thingIndex < GC_PAGE_SIZE);\\n-    if (thingIndex >= (offsetInArena & GC_PAGE_MASK))\\n-        thingIndex += GC_THINGS_SIZE;\\n-    return (uint8 *)pi - offsetInArena + thingIndex;\\n+    JSGCArenaInfo *a;\\n+    uint32 index;\\n+\\n+    a = THING_TO_ARENA(thing);\\n+    index = THING_TO_INDEX(thing, a->list->thingSize);\\n+    return THING_FLAGP(a, index);\\n }\\n \\n JSRuntime*\\n js_GetGCStringRuntime(JSString *str)\\n {\\n-    JSGCPageInfo *pi;\\n     JSGCArenaList *list;\\n \\n-    pi = THING_TO_PAGE(str);\\n-    list = PAGE_TO_ARENA(pi)->list;\\n+    list = THING_TO_ARENA(str)->list;\\n \\n     JS_ASSERT(list->thingSize == sizeof(JSGCThing));\\n     JS_ASSERT(GC_FREELIST_INDEX(sizeof(JSGCThing)) == 0);\\n@@ -623,11 +894,59 @@ typedef struct JSGCRootHashEntry {\\n \\n /* Initial size of the gcRootsHash table (SWAG, small enough to amortize). */\\n #define GC_ROOTS_SIZE   256\\n-#define GC_FINALIZE_LEN 1024\\n+\\n+/*\\n+ * For a CPU with extremely large pages using them for GC things wastes\\n+ * too much memory.\\n+ */\\n+#define GC_ARENAS_PER_CPU_PAGE_LIMIT JS_BIT(18 - GC_ARENA_SHIFT)\\n+\\n+JS_STATIC_ASSERT(GC_ARENAS_PER_CPU_PAGE_LIMIT <= NO_FREE_ARENAS);\\n \\n JSBool\\n js_InitGC(JSRuntime *rt, uint32 maxbytes)\\n {\\n+#if JS_GC_USE_MMAP\\n+    if (js_gcArenasPerChunk == 0) {\\n+        size_t cpuPageSize, arenasPerPage;\\n+# if defined(XP_WIN)\\n+        SYSTEM_INFO si;\\n+\\n+        GetSystemInfo(&si);\\n+        cpuPageSize = si.dwPageSize;\\n+\\n+# elif defined(XP_UNIX) || defined(XP_BEOS)\\n+        cpuPageSize = (size_t) sysconf(_SC_PAGESIZE);\\n+# else\\n+#  error \\\"Not implemented\\\"\\n+# endif\\n+        /* cpuPageSize is a power of 2. */\\n+        JS_ASSERT((cpuPageSize & (cpuPageSize - 1)) == 0);\\n+        arenasPerPage = cpuPageSize >> GC_ARENA_SHIFT;\\n+#ifdef DEBUG\\n+        if (arenasPerPage == 0) {\\n+            fprintf(stderr,\\n+\\\"JS engine warning: the size of the CPU page, %u bytes, is too low to use\\\\n\\\"\\n+\\\"paged allocation for the garbage collector. Please report this.\\\\n\\\",\\n+                    (unsigned) cpuPageSize);\\n+        }\\n+#endif\\n+        if (arenasPerPage - 1 <= (size_t) (GC_ARENAS_PER_CPU_PAGE_LIMIT - 1)) {\\n+            /*\\n+             * Use at least 4 GC arenas per paged allocation chunk to minimize\\n+             * the overhead of mmap/VirtualAlloc.\\n+             */\\n+            js_gcUseMmap = JS_TRUE;\\n+            js_gcArenasPerChunk = JS_MAX((uint32) arenasPerPage, 4);\\n+        } else {\\n+            js_gcUseMmap = JS_FALSE;\\n+            js_gcArenasPerChunk = 7;\\n+        }\\n+    }\\n+#endif\\n+    JS_ASSERT(1 <= js_gcArenasPerChunk &&\\n+              js_gcArenasPerChunk <= NO_FREE_ARENAS);\\n+\\n     InitGCArenaLists(rt);\\n     if (!JS_DHashTableInit(&rt->gcRootsHash, JS_DHashGetStubOps(), NULL,\\n                            sizeof(JSGCRootHashEntry), GC_ROOTS_SIZE)) {\\n@@ -650,6 +969,7 @@ JS_FRIEND_API(void)\\n js_DumpGCStats(JSRuntime *rt, FILE *fp)\\n {\\n     uintN i;\\n+    size_t thingsPerArena;\\n     size_t totalThings, totalMaxThings, totalBytes;\\n     size_t sumArenas, sumTotalArenas;\\n     size_t sumFreeSize, sumTotalFreeSize;\\n@@ -673,6 +993,7 @@ js_DumpGCStats(JSRuntime *rt, FILE *fp)\\n                     i, UL(GC_FREELIST_NBYTES(i)));\\n             continue;\\n         }\\n+        thingsPerArena = THINGS_PER_ARENA(list->thingSize);\\n         fprintf(fp, \\\"ARENA LIST %u (thing size %lu):\\\\n\\\",\\n                 i, UL(GC_FREELIST_NBYTES(i)));\\n         fprintf(fp, \\\"                     arenas: %lu\\\\n\\\", UL(stats->narenas));\\n@@ -683,20 +1004,18 @@ js_DumpGCStats(JSRuntime *rt, FILE *fp)\\n         fprintf(fp, \\\"          free list density: %.1f%%\\\\n\\\",\\n                 stats->narenas == 0\\n                 ? 0.0\\n-                : (100.0 * list->thingSize * (jsdouble)stats->freelen /\\n-                   (GC_THINGS_SIZE * (jsdouble)stats->narenas)));\\n+                : 100.0 * stats->freelen / (thingsPerArena * stats->narenas));\\n         fprintf(fp, \\\"  average free list density: %.1f%%\\\\n\\\",\\n                 stats->totalarenas == 0\\n                 ? 0.0\\n-                : (100.0 * list->thingSize * (jsdouble)stats->totalfreelen /\\n-                   (GC_THINGS_SIZE * (jsdouble)stats->totalarenas)));\\n+                : 100.0 * stats->totalfreelen /\\n+                  (thingsPerArena * stats->totalarenas));\\n         fprintf(fp, \\\"                   recycles: %lu\\\\n\\\", UL(stats->recycle));\\n         fprintf(fp, \\\"        recycle/alloc ratio: %.2f\\\\n\\\",\\n-                (jsdouble)stats->recycle /\\n-                (jsdouble)(stats->totalnew - stats->recycle));\\n+                (double) stats->recycle / (stats->totalnew - stats->recycle));\\n         totalThings += stats->nthings;\\n         totalMaxThings += stats->maxthings;\\n-        totalBytes += GC_FREELIST_NBYTES(i) * stats->nthings;\\n+        totalBytes += list->thingSize * stats->nthings;\\n         sumArenas += stats->narenas;\\n         sumTotalArenas += stats->totalarenas;\\n         sumFreeSize += list->thingSize * stats->freelen;\\n@@ -715,11 +1034,11 @@ js_DumpGCStats(JSRuntime *rt, FILE *fp)\\n     fprintf(fp, \\\"    total free list density: %.1f%%\\\\n\\\",\\n             sumArenas == 0\\n             ? 0.0\\n-            : 100.0 * sumFreeSize / (GC_THINGS_SIZE * (jsdouble)sumArenas));\\n+            : 100.0 * sumFreeSize / (sumArenas << GC_ARENA_SHIFT));\\n     fprintf(fp, \\\"  average free list density: %.1f%%\\\\n\\\",\\n             sumTotalFreeSize == 0\\n             ? 0.0\\n-            : 100.0 * sumTotalFreeSize / (GC_THINGS_SIZE * sumTotalArenas));\\n+            : 100.0 * sumTotalFreeSize / (sumTotalArenas << GC_ARENA_SHIFT));\\n     fprintf(fp, \\\"allocation retries after GC: %lu\\\\n\\\", ULSTAT(retry));\\n     fprintf(fp, \\\"        allocation failures: %lu\\\\n\\\", ULSTAT(fail));\\n     fprintf(fp, \\\"         things born locked: %lu\\\\n\\\", ULSTAT(lockborn));\\n@@ -729,9 +1048,9 @@ js_DumpGCStats(JSRuntime *rt, FILE *fp)\\n     fprintf(fp, \\\"     maximum mark recursion: %lu\\\\n\\\", ULSTAT(maxdepth));\\n     fprintf(fp, \\\"     mark C recursion depth: %lu\\\\n\\\", ULSTAT(cdepth));\\n     fprintf(fp, \\\"   maximum mark C recursion: %lu\\\\n\\\", ULSTAT(maxcdepth));\\n-    fprintf(fp, \\\"      delayed scan bag adds: %lu\\\\n\\\", ULSTAT(unscanned));\\n+    fprintf(fp, \\\"      delayed tracing calls: %lu\\\\n\\\", ULSTAT(untraced));\\n #ifdef DEBUG\\n-    fprintf(fp, \\\"  max delayed scan bag size: %lu\\\\n\\\", ULSTAT(maxunscanned));\\n+    fprintf(fp, \\\"      max trace later count: %lu\\\\n\\\", ULSTAT(maxuntraced));\\n #endif\\n     fprintf(fp, \\\"   maximum GC nesting level: %lu\\\\n\\\", ULSTAT(maxlevel));\\n     fprintf(fp, \\\"potentially useful GC calls: %lu\\\\n\\\", ULSTAT(poke));\\n@@ -1031,10 +1350,10 @@ js_NewGCThing(JSContext *cx, uintN flags, size_t nbytes)\\n     uintN flindex;\\n     JSBool doGC;\\n     JSGCThing *thing;\\n-    uint8 *flagp, *firstPage;\\n+    uint8 *flagp;\\n     JSGCArenaList *arenaList;\\n-    jsuword offset;\\n-    JSGCArena *a;\\n+    JSGCArenaInfo *a;\\n+    uintN thingsLimit;\\n     JSLocalRootStack *lrs;\\n #ifdef JS_THREADSAFE\\n     JSBool gcLocked;\\n@@ -1145,67 +1464,71 @@ js_NewGCThing(JSContext *cx, uintN flags, size_t nbytes)\\n             break;\\n         }\\n \\n-        /* Allocate from the tail of last arena or from new arena if we can. */\\n-        if ((arenaList->last && arenaList->lastLimit != GC_THINGS_SIZE) ||\\n-            NewGCArena(rt, arenaList)) {\\n-\\n-            offset = arenaList->lastLimit;\\n-            if ((offset & GC_PAGE_MASK) == 0) {\\n-                /*\\n-                 * Skip JSGCPageInfo record located at GC_PAGE_SIZE boundary.\\n-                 */\\n-                offset += PAGE_THING_GAP(nbytes);\\n-            }\\n-            JS_ASSERT(offset + nbytes <= GC_THINGS_SIZE);\\n-            arenaList->lastLimit = (uint16)(offset + nbytes);\\n+        /*\\n+         * Try to allocate things from the last arena. If it is fully used,\\n+         * check if we can allocate a new one and, if we cannot, consider\\n+         * doing a \\\"last ditch\\\" GC unless already tried.\\n+         */\\n+        thingsLimit = THINGS_PER_ARENA(nbytes);\\n+        if (arenaList->lastCount != thingsLimit) {\\n+            JS_ASSERT(arenaList->lastCount < thingsLimit);\\n             a = arenaList->last;\\n-            firstPage = (uint8 *)FIRST_THING_PAGE(a);\\n-            thing = (JSGCThing *)(firstPage + offset);\\n-            flagp = a->base + offset / sizeof(JSGCThing);\\n-            if (flagp >= firstPage)\\n-                flagp += GC_THINGS_SIZE;\\n+        } else {\\n+            if (rt->gcBytes >= rt->gcMaxBytes || !(a = NewGCArena(rt))) {\\n+                if (doGC)\\n+                    goto fail;\\n+                rt->gcPoke = JS_TRUE;\\n+                doGC = JS_TRUE;\\n+                continue;\\n+            }\\n+\\n+            rt->gcBytes += GC_ARENA_SIZE;\\n+            METER(++arenaList->stats.narenas);\\n+            METER(arenaList->stats.maxarenas\\n+                  = JS_MAX(arenaList->stats.maxarenas,\\n+                           arenaList->stats.narenas));\\n+\\n+            a->list = arenaList;\\n+            a->prev = arenaList->last;\\n+            a->prevUntracedPage = 0;\\n+            a->untracedThings = 0;\\n+            arenaList->last = a;\\n+            arenaList->lastCount = 0;\\n+        }\\n+\\n+        flagp = THING_FLAGP(a, arenaList->lastCount);\\n+        thing = (JSGCThing *) FLAGP_TO_THING(flagp, nbytes);\\n+        arenaList->lastCount++;\\n \\n #ifdef JS_THREADSAFE\\n-            /*\\n-             * Refill the local free list by taking free things from the last\\n-             * arena. Prefer to order free things by ascending address in the\\n-             * (unscientific) hope of better cache locality.\\n-             */\\n-            if (rt->gcMallocBytes >= rt->gcMaxMallocBytes || flbase[flindex])\\n-                break;\\n-            METER(nfree = 0);\\n-            lastptr = &flbase[flindex];\\n-            maxFreeThings = MAX_THREAD_LOCAL_THINGS;\\n-            for (offset = arenaList->lastLimit;\\n-                 offset != GC_THINGS_SIZE && maxFreeThings-- != 0;\\n-                 offset += nbytes) {\\n-                if ((offset & GC_PAGE_MASK) == 0)\\n-                    offset += PAGE_THING_GAP(nbytes);\\n-                JS_ASSERT(offset + nbytes <= GC_THINGS_SIZE);\\n-                tmpflagp = a->base + offset / sizeof(JSGCThing);\\n-                if (tmpflagp >= firstPage)\\n-                    tmpflagp += GC_THINGS_SIZE;\\n-\\n-                tmpthing = (JSGCThing *)(firstPage + offset);\\n-                tmpthing->flagp = tmpflagp;\\n-                *tmpflagp = GCF_FINAL;    /* signifying that thing is free */\\n-\\n-                *lastptr = tmpthing;\\n-                lastptr = &tmpthing->next;\\n-                METER(++nfree);\\n-            }\\n-            arenaList->lastLimit = (uint16)offset;\\n-            *lastptr = NULL;\\n-            METER(arenaList->stats.freelen += nfree);\\n-#endif\\n+        /*\\n+         * Refill the local free list by taking free things from the last\\n+         * arena. Prefer to order free things by ascending address in the\\n+         * (unscientific) hope of better cache locality.\\n+         */\\n+        if (rt->gcMallocBytes >= rt->gcMaxMallocBytes || flbase[flindex])\\n             break;\\n+        METER(nfree = 0);\\n+        lastptr = &flbase[flindex];\\n+        maxFreeThings = thingsLimit - arenaList->lastCount;\\n+        if (maxFreeThings > MAX_THREAD_LOCAL_THINGS)\\n+            maxFreeThings = MAX_THREAD_LOCAL_THINGS;\\n+        METER(arenaList->stats.freelen += maxFreeThings);\\n+        while (maxFreeThings != 0) {\\n+            --maxFreeThings;\\n+\\n+            tmpflagp = THING_FLAGP(a, arenaList->lastCount);\\n+            tmpthing = (JSGCThing *) FLAGP_TO_THING(tmpflagp, nbytes);\\n+            arenaList->lastCount++;\\n+            tmpthing->flagp = tmpflagp;\\n+            *tmpflagp = GCF_FINAL;    /* signifying that thing is free */\\n+\\n+            *lastptr = tmpthing;\\n+            lastptr = &tmpthing->next;\\n         }\\n-\\n-        /* Consider doing a \\\"last ditch\\\" GC unless already tried. */\\n-        if (doGC)\\n-            goto fail;\\n-        rt->gcPoke = JS_TRUE;\\n-        doGC = JS_TRUE;\\n+        *lastptr = NULL;\\n+#endif\\n+        break;\\n     }\\n \\n     /* We successfully allocated the thing. */\\n@@ -1473,234 +1796,171 @@ JS_TraceChildren(JSTracer *trc, void *thing, uint32 kind)\\n }\\n \\n /*\\n- * Avoid using PAGE_THING_GAP inside this macro to optimize the\\n- * thingsPerUnscannedChunk calculation when thingSize is a power of two.\\n+ * Number of things covered by a single bit of JSGCArenaInfo.untracedThings.\\n  */\\n-#define GET_GAP_AND_CHUNK_SPAN(thingSize, thingsPerUnscannedChunk, pageGap)   \\\\\\n-    JS_BEGIN_MACRO                                                            \\\\\\n-        if (0 == ((thingSize) & ((thingSize) - 1))) {                         \\\\\\n-            pageGap = (thingSize);                                            \\\\\\n-            thingsPerUnscannedChunk = ((GC_PAGE_SIZE / (thingSize))           \\\\\\n-                                       + JS_BITS_PER_WORD - 1)                \\\\\\n-                                      >> JS_BITS_PER_WORD_LOG2;               \\\\\\n-        } else {                                                              \\\\\\n-            pageGap = GC_PAGE_SIZE % (thingSize);                             \\\\\\n-            thingsPerUnscannedChunk = JS_HOWMANY(GC_PAGE_SIZE / (thingSize),  \\\\\\n-                                                 JS_BITS_PER_WORD);           \\\\\\n-        }                                                                     \\\\\\n-    JS_END_MACRO\\n+#define THINGS_PER_UNTRACED_BIT(thingSize)                                    \\\\\\n+    JS_HOWMANY(THINGS_PER_ARENA(thingSize), JS_BITS_PER_WORD)\\n \\n static void\\n-AddThingToUnscannedBag(JSRuntime *rt, void *thing, uint8 *flagp)\\n+DelayTracingChildren(JSRuntime *rt, uint8 *flagp)\\n {\\n-    JSGCPageInfo *pi;\\n-    JSGCArena *arena;\\n-    size_t thingSize;\\n-    size_t thingsPerUnscannedChunk;\\n-    size_t pageGap;\\n-    size_t chunkIndex;\\n+    JSGCArenaInfo *a;\\n+    uint32 untracedBitIndex;\\n     jsuword bit;\\n \\n-    /* Things from delayed scanning bag are marked as GCF_MARK | GCF_FINAL. */\\n+    /*\\n+     * Things with children to be traced later are marked with\\n+     * GCF_MARK | GCF_FINAL flags.\\n+     */\\n     JS_ASSERT((*flagp & (GCF_MARK | GCF_FINAL)) == GCF_MARK);\\n     *flagp |= GCF_FINAL;\\n \\n-    METER(rt->gcStats.unscanned++);\\n+    METER(rt->gcStats.untraced++);\\n #ifdef DEBUG\\n-    ++rt->gcUnscannedBagSize;\\n-    METER(if (rt->gcUnscannedBagSize > rt->gcStats.maxunscanned)\\n-              rt->gcStats.maxunscanned = rt->gcUnscannedBagSize);\\n+    ++rt->gcTraceLaterCount;\\n+    METER(if (rt->gcTraceLaterCount > rt->gcStats.maxuntraced)\\n+              rt->gcStats.maxuntraced = rt->gcTraceLaterCount);\\n #endif\\n \\n-    pi = THING_TO_PAGE(thing);\\n-    arena = PAGE_TO_ARENA(pi);\\n-    thingSize = arena->list->thingSize;\\n-    GET_GAP_AND_CHUNK_SPAN(thingSize, thingsPerUnscannedChunk, pageGap);\\n-    chunkIndex = (((jsuword)thing & GC_PAGE_MASK) - pageGap) /\\n-                 (thingSize * thingsPerUnscannedChunk);\\n-    JS_ASSERT(chunkIndex < JS_BITS_PER_WORD);\\n-    bit = (jsuword)1 << chunkIndex;\\n-    if (pi->unscannedBitmap != 0) {\\n-        JS_ASSERT(rt->gcUnscannedArenaStackTop);\\n-        if (thingsPerUnscannedChunk != 1) {\\n-            if (pi->unscannedBitmap & bit) {\\n-                /* Chunk already contains things to scan later. */\\n-                return;\\n-            }\\n-        } else {\\n-            /*\\n-             * The chunk must not contain things to scan later if there is\\n-             * only one thing per chunk.\\n-             */\\n-            JS_ASSERT(!(pi->unscannedBitmap & bit));\\n+    a = FLAGP_TO_ARENA(flagp);\\n+    untracedBitIndex = FLAGP_TO_INDEX(flagp) /\\n+                       THINGS_PER_UNTRACED_BIT(a->list->thingSize);\\n+    JS_ASSERT(untracedBitIndex < JS_BITS_PER_WORD);\\n+    bit = (jsuword)1 << untracedBitIndex;\\n+    if (a->untracedThings != 0) {\\n+        JS_ASSERT(rt->gcUntracedArenaStackTop);\\n+        if (a->untracedThings & bit) {\\n+            /* bit already covers things with children to trace later. */\\n+            return;\\n         }\\n-        pi->unscannedBitmap |= bit;\\n-        JS_ASSERT(arena->unscannedPages & ((size_t)1 << PAGE_INDEX(pi)));\\n+        a->untracedThings |= bit;\\n     } else {\\n         /*\\n-         * The thing is the first unscanned thing in the page, set the bit\\n-         * corresponding to this page arena->unscannedPages.\\n+         * The thing is the first thing with not yet traced children in the\\n+         * whole arena, so push the arena on the stack of arenas with things\\n+         * to be traced later unless the arena has already been pushed. We\\n+         * detect that through checking prevUntracedPage as the field is 0\\n+         * only for not yet pushed arenas. To ensure that\\n+         *   prevUntracedPage != 0\\n+         * even when the stack contains one element, we make prevUntracedPage\\n+         * for the arena at the bottom to point to itself.\\n+         *\\n+         * See comments in TraceDelayedChildren.\\n          */\\n-        pi->unscannedBitmap = bit;\\n-        JS_ASSERT(PAGE_INDEX(pi) < JS_BITS_PER_WORD);\\n-        bit = (jsuword)1 << PAGE_INDEX(pi);\\n-        JS_ASSERT(!(arena->unscannedPages & bit));\\n-        if (arena->unscannedPages != 0) {\\n-            arena->unscannedPages |= bit;\\n-            JS_ASSERT(arena->prevUnscanned);\\n-            JS_ASSERT(rt->gcUnscannedArenaStackTop);\\n-        } else {\\n-            /*\\n-             * The thing is the first unscanned thing in the whole arena, push\\n-             * the arena on the stack of unscanned arenas unless the arena\\n-             * has already been pushed. We detect that through prevUnscanned\\n-             * field which is NULL only for not yet pushed arenas. To ensure\\n-             * that prevUnscanned != NULL even when the stack contains one\\n-             * element, we make prevUnscanned for the arena at the bottom\\n-             * to point to itself.\\n-             *\\n-             * See comments in ScanDelayedChildren.\\n-             */\\n-            arena->unscannedPages = bit;\\n-            if (!arena->prevUnscanned) {\\n-                if (!rt->gcUnscannedArenaStackTop) {\\n-                    /* Stack was empty, mark the arena as bottom element. */\\n-                    arena->prevUnscanned = arena;\\n-                } else {\\n-                    JS_ASSERT(rt->gcUnscannedArenaStackTop->prevUnscanned);\\n-                    arena->prevUnscanned = rt->gcUnscannedArenaStackTop;\\n-                }\\n-                rt->gcUnscannedArenaStackTop = arena;\\n+        a->untracedThings = bit;\\n+        if (a->prevUntracedPage == 0) {\\n+            if (!rt->gcUntracedArenaStackTop) {\\n+                /* Stack was empty, mark the arena as the bottom element. */\\n+                a->prevUntracedPage = ARENA_INFO_TO_PAGE(a);\\n+            } else {\\n+                JS_ASSERT(rt->gcUntracedArenaStackTop->prevUntracedPage != 0);\\n+                a->prevUntracedPage =\\n+                    ARENA_INFO_TO_PAGE(rt->gcUntracedArenaStackTop);\\n             }\\n-         }\\n-     }\\n-    JS_ASSERT(rt->gcUnscannedArenaStackTop);\\n+            rt->gcUntracedArenaStackTop = a;\\n+        }\\n+    }\\n+    JS_ASSERT(rt->gcUntracedArenaStackTop);\\n }\\n \\n static void\\n-ScanDelayedChildren(JSTracer *trc)\\n+TraceDelayedChildren(JSTracer *trc)\\n {\\n     JSRuntime *rt;\\n-    JSGCArena *arena;\\n-    size_t thingSize;\\n-    size_t thingsPerUnscannedChunk;\\n-    size_t pageGap;\\n-    size_t pageIndex;\\n-    JSGCPageInfo *pi;\\n-    size_t chunkIndex;\\n-    size_t thingOffset, thingLimit;\\n+    JSGCArenaInfo *a, *aprev;\\n+    uint32 thingSize;\\n+    uint32 thingsPerUntracedBit;\\n+    uint32 untracedBitIndex, thingIndex, indexLimit, endIndex;\\n     JSGCThing *thing;\\n     uint8 *flagp;\\n-    JSGCArena *prevArena;\\n \\n     rt = trc->context->runtime;\\n-    arena = rt->gcUnscannedArenaStackTop;\\n-    if (!arena) {\\n-        JS_ASSERT(rt->gcUnscannedBagSize == 0);\\n+    a = rt->gcUntracedArenaStackTop;\\n+    if (!a) {\\n+        JS_ASSERT(rt->gcTraceLaterCount == 0);\\n         return;\\n     }\\n \\n-  init_size:\\n-    thingSize = arena->list->thingSize;\\n-    GET_GAP_AND_CHUNK_SPAN(thingSize, thingsPerUnscannedChunk, pageGap);\\n     for (;;) {\\n         /*\\n-         * The following assert verifies that the current arena belongs to\\n-         * the unscan stack since AddThingToUnscannedBag ensures that even\\n-         * for stack's bottom prevUnscanned != NULL but rather points to self.\\n+         * The following assert verifies that the current arena belongs to the\\n+         * untraced stack, since DelayTracingChildren ensures that even for\\n+         * stack's bottom prevUntracedPage != 0 but rather points to itself.\\n          */\\n-        JS_ASSERT(arena->prevUnscanned);\\n-        JS_ASSERT(rt->gcUnscannedArenaStackTop->prevUnscanned);\\n-        while (arena->unscannedPages != 0) {\\n-            pageIndex = JS_FLOOR_LOG2W(arena->unscannedPages);\\n-            JS_ASSERT(pageIndex < GC_PAGE_COUNT);\\n-            pi = (JSGCPageInfo *)(FIRST_THING_PAGE(arena) +\\n-                                  pageIndex * GC_PAGE_SIZE);\\n-            JS_ASSERT(pi->unscannedBitmap);\\n-            chunkIndex = JS_FLOOR_LOG2W(pi->unscannedBitmap);\\n-            pi->unscannedBitmap &= ~((jsuword)1 << chunkIndex);\\n-            if (pi->unscannedBitmap == 0)\\n-                arena->unscannedPages &= ~((jsuword)1 << pageIndex);\\n-            thingOffset = (pageGap\\n-                           + chunkIndex * thingsPerUnscannedChunk * thingSize);\\n-            JS_ASSERT(thingOffset >= sizeof(JSGCPageInfo));\\n-            thingLimit = thingOffset + thingsPerUnscannedChunk * thingSize;\\n-            if (thingsPerUnscannedChunk != 1) {\\n-                /*\\n-                 * thingLimit can go beyond the last allocated thing for the\\n-                 * last chunk as the real limit can be inside the chunk.\\n-                 */\\n-                if (arena->list->last == arena &&\\n-                    arena->list->lastLimit < (pageIndex * GC_PAGE_SIZE +\\n-                                              thingLimit)) {\\n-                    thingLimit = (arena->list->lastLimit -\\n-                                  pageIndex * GC_PAGE_SIZE);\\n-                } else if (thingLimit > GC_PAGE_SIZE) {\\n-                    thingLimit = GC_PAGE_SIZE;\\n-                }\\n-                JS_ASSERT(thingLimit > thingOffset);\\n-            }\\n-            JS_ASSERT(arena->list->last != arena ||\\n-                      arena->list->lastLimit >= (pageIndex * GC_PAGE_SIZE +\\n-                                                 thingLimit));\\n-            JS_ASSERT(thingLimit <= GC_PAGE_SIZE);\\n+        JS_ASSERT(a->prevUntracedPage != 0);\\n+        JS_ASSERT(rt->gcUntracedArenaStackTop->prevUntracedPage != 0);\\n+        thingSize = a->list->thingSize;\\n+        indexLimit = (a == a->list->last)\\n+                     ? a->list->lastCount\\n+                     : THINGS_PER_ARENA(thingSize);\\n+        thingsPerUntracedBit = THINGS_PER_UNTRACED_BIT(thingSize);\\n \\n-            for (; thingOffset != thingLimit; thingOffset += thingSize) {\\n+        /*\\n+         * We can not use do-while loop here as a->untracedThings can be zero\\n+         * before the loop as a leftover from the previous iterations. See\\n+         * comments after the loop.\\n+         */\\n+        while (a->untracedThings != 0) {\\n+            untracedBitIndex = JS_FLOOR_LOG2W(a->untracedThings);\\n+            a->untracedThings &= ~((jsuword)1 << untracedBitIndex);\\n+            thingIndex = untracedBitIndex * thingsPerUntracedBit;\\n+            endIndex = thingIndex + thingsPerUntracedBit;\\n+\\n+            /*\\n+             * endIndex can go beyond the last allocated thing as the real\\n+             * limit can be \\\"inside\\\" the bit.\\n+             */\\n+            if (endIndex > indexLimit)\\n+                endIndex = indexLimit;\\n+            JS_ASSERT(thingIndex < indexLimit);\\n+\\n+            do {\\n                 /*\\n-                 * XXX: inline js_GetGCThingFlags() to use already available\\n-                 * pi.\\n+                 * Skip free or already traced things that share the bit\\n+                 * with untraced ones.\\n                  */\\n-                thing = (JSGCThing *)((jsuword)pi + thingOffset);\\n-                flagp = js_GetGCThingFlags(thing);\\n-                if (thingsPerUnscannedChunk != 1) {\\n-                    /*\\n-                     * Skip free or already scanned things that share the chunk\\n-                     * with unscanned ones.\\n-                     */\\n-                    if ((*flagp & (GCF_MARK|GCF_FINAL)) != (GCF_MARK|GCF_FINAL))\\n-                        continue;\\n-                }\\n-                JS_ASSERT((*flagp & (GCF_MARK|GCF_FINAL))\\n-                              == (GCF_MARK|GCF_FINAL));\\n+                flagp = THING_FLAGP(a, thingIndex);\\n+                if ((*flagp & (GCF_MARK|GCF_FINAL)) != (GCF_MARK|GCF_FINAL))\\n+                    continue;\\n                 *flagp &= ~GCF_FINAL;\\n #ifdef DEBUG\\n-                JS_ASSERT(rt->gcUnscannedBagSize != 0);\\n-                --rt->gcUnscannedBagSize;\\n+                JS_ASSERT(rt->gcTraceLaterCount != 0);\\n+                --rt->gcTraceLaterCount;\\n #endif\\n+                thing = FLAGP_TO_THING(flagp, thingSize);\\n                 JS_TraceChildren(trc, thing,\\n                                  GCTypeToTraceKindMap[*flagp & GCF_TYPEMASK]);\\n-            }\\n+            } while (++thingIndex != endIndex);\\n         }\\n+\\n         /*\\n-         * We finished scanning of the arena but we can only pop it from\\n-         * the stack if the arena is the stack's top.\\n+         * We finished tracing of all things in the the arena but we can only\\n+         * pop it from the stack if the arena is the stack's top.\\n          *\\n-         * When JS_TraceChildren from the above calls JS_Trace that in turn\\n-         * on low C stack calls AddThingToUnscannedBag and the latter pushes\\n-         * new arenas to the unscanned stack, we have to skip popping of this\\n-         * arena until it becomes the top of the stack again.\\n+         * When JS_TraceChildren from the above calls JS_CallTracer that in\\n+         * turn on low C stack calls DelayTracingChildren and the latter\\n+         * pushes new arenas to the untraced stack, we have to skip popping\\n+         * of this arena until it becomes the top of the stack again.\\n          */\\n-        if (arena == rt->gcUnscannedArenaStackTop) {\\n-            prevArena = arena->prevUnscanned;\\n-            arena->prevUnscanned = NULL;\\n-            if (arena == prevArena) {\\n+        if (a == rt->gcUntracedArenaStackTop) {\\n+            aprev = ARENA_PAGE_TO_INFO(a->prevUntracedPage);\\n+            a->prevUntracedPage = 0;\\n+            if (a == aprev) {\\n                 /*\\n-                 * prevUnscanned points to itself and we reached the bottom\\n-                 * of the stack.\\n+                 * prevUntracedPage points to itself and we reached the\\n+                 * bottom of the stack.\\n                  */\\n                 break;\\n             }\\n-            rt->gcUnscannedArenaStackTop = arena = prevArena;\\n+            rt->gcUntracedArenaStackTop = a = aprev;\\n         } else {\\n-            arena = rt->gcUnscannedArenaStackTop;\\n+            a = rt->gcUntracedArenaStackTop;\\n         }\\n-        if (arena->list->thingSize != thingSize)\\n-            goto init_size;\\n     }\\n-    JS_ASSERT(rt->gcUnscannedArenaStackTop);\\n-    JS_ASSERT(!rt->gcUnscannedArenaStackTop->prevUnscanned);\\n-    rt->gcUnscannedArenaStackTop = NULL;\\n-    JS_ASSERT(rt->gcUnscannedBagSize == 0);\\n+    JS_ASSERT(rt->gcUntracedArenaStackTop);\\n+    JS_ASSERT(rt->gcUntracedArenaStackTop->prevUntracedPage == 0);\\n+    rt->gcUntracedArenaStackTop = NULL;\\n+    JS_ASSERT(rt->gcTraceLaterCount == 0);\\n }\\n \\n JS_PUBLIC_API(void)\\n@@ -1724,8 +1984,42 @@ JS_CallTracer(JSTracer *trc, void *thing, uint32 kind)\\n     JS_ASSERT(rt->gcMarkingTracer == trc);\\n     JS_ASSERT(rt->gcLevel > 0);\\n \\n+    /*\\n+     * Optimize for string and double as their size is known and their tracing\\n+     * is not recursive.\\n+     */\\n+    switch (kind) {\\n+      case JSTRACE_DOUBLE:\\n+        flagp = THING_TO_FLAGP(thing, sizeof(JSGCThing));\\n+        JS_ASSERT((*flagp & GCF_FINAL) == 0);\\n+        JS_ASSERT(GCTypeToTraceKindMap[*flagp & GCF_TYPEMASK] == kind);\\n+        if (rt->gcThingCallback)\\n+            rt->gcThingCallback(thing, *flagp, rt->gcThingCallbackClosure);\\n+\\n+        *flagp |= GCF_MARK;\\n+        goto out;\\n+\\n+      case JSTRACE_STRING:\\n+        for (;;) {\\n+            flagp = THING_TO_FLAGP(thing, sizeof(JSGCThing));\\n+            JS_ASSERT((*flagp & GCF_FINAL) == 0);\\n+            JS_ASSERT(GCTypeToTraceKindMap[*flagp & GCF_TYPEMASK] == kind);\\n+            if (rt->gcThingCallback)\\n+                rt->gcThingCallback(thing, *flagp, rt->gcThingCallbackClosure);\\n+\\n+            if (!JSSTRING_IS_DEPENDENT((JSString *) thing)) {\\n+                *flagp |= GCF_MARK;\\n+                goto out;\\n+            }\\n+            if (*flagp & GCF_MARK)\\n+                goto out;\\n+            *flagp |= GCF_MARK;\\n+            thing = JSSTRDEP_BASE((JSString *) thing);\\n+        }\\n+        /* NOTREACHED */\\n+    }\\n+\\n     flagp = js_GetGCThingFlags(thing);\\n-    JS_ASSERT(*flagp != GCF_FINAL);\\n     JS_ASSERT(GCTypeToTraceKindMap[*flagp & GCF_TYPEMASK] == kind);\\n \\n     if (rt->gcThingCallback)\\n@@ -1733,8 +2027,13 @@ JS_CallTracer(JSTracer *trc, void *thing, uint32 kind)\\n \\n     if (*flagp & GCF_MARK)\\n         goto out;\\n-    *flagp |= GCF_MARK;\\n \\n+    /*\\n+     * We check for non-final flag only if mark is unset as\\n+     * DelayTracingChildren uses the flag. See comments in the function.\\n+     */\\n+    JS_ASSERT(*flagp != GCF_FINAL);\\n+    *flagp |= GCF_MARK;\\n     if (!cx->insideGCMarkCallback) {\\n         /*\\n          * With JS_GC_ASSUME_LOW_C_STACK defined the mark phase of GC always\\n@@ -1748,28 +2047,28 @@ JS_CallTracer(JSTracer *trc, void *thing, uint32 kind)\\n # define RECURSION_TOO_DEEP() (!JS_CHECK_STACK_SIZE(cx, stackDummy))\\n #endif\\n         if (RECURSION_TOO_DEEP())\\n-            AddThingToUnscannedBag(rt, thing, flagp);\\n+            DelayTracingChildren(rt, flagp);\\n         else\\n             JS_TraceChildren(trc, thing, kind);\\n     } else {\\n         /*\\n          * For API compatibility we allow for the callback to assume that\\n-         * after it calls JS_Trace or JS_MarkGCThing for the last time, the\\n-         * callback can start to finalize its own objects that are only\\n-         * referenced by unmarked GC things.\\n+         * after it calls JS_MarkGCThing for the last time, the callback can\\n+         * start to finalize its own objects that are only referenced by\\n+         * unmarked GC things.\\n          *\\n          * Since we do not know which call from inside the callback is the\\n-         * last, we ensure that the unscanned bag is always empty when we\\n-         * return to the callback and all marked things are scanned.\\n+         * last, we ensure that children of all marked things are traced and\\n+         * call TraceDelayedChildren(trc) after tracing the thing.\\n          *\\n-         * We do not check for the stack size here and uncondinally call\\n-         * JS_TraceChildren. Otherwise with low C stack the thing would be\\n-         * pushed to the bag just to be feed again to JS_TraceChildren from\\n-         * inside ScanDelayedChildren.\\n+         * As TraceDelayedChildren unconditionally invokes JS_TraceChildren\\n+         * for the things with untraced children, calling DelayTracingChildren\\n+         * is useless here. Hence we always trace thing's children even with a\\n+         * low native stack.\\n          */\\n         cx->insideGCMarkCallback = JS_FALSE;\\n         JS_TraceChildren(trc, thing, kind);\\n-        ScanDelayedChildren(trc);\\n+        TraceDelayedChildren(trc);\\n         cx->insideGCMarkCallback = JS_TRUE;\\n     }\\n \\n@@ -1816,18 +2115,20 @@ gc_root_traversal(JSDHashTable *table, JSDHashEntryHdr *hdr, uint32 num,\\n         jsuword thing = (jsuword) JSVAL_TO_GCTHING(v);\\n         uintN i;\\n         JSGCArenaList *arenaList;\\n-        JSGCArena *a;\\n+        uint32 thingSize;\\n+        JSGCArenaInfo *a;\\n         size_t limit;\\n \\n         for (i = 0; i < GC_NUM_FREELISTS; i++) {\\n             arenaList = &trc->context->runtime->gcArenaList[i];\\n-            limit = arenaList->lastLimit;\\n+            thingSize = arenaList->thingSize;\\n+            limit = (size_t) arenaList->lastCount * thingSize;\\n             for (a = arenaList->last; a; a = a->prev) {\\n-                if (thing - FIRST_THING_PAGE(a) < limit) {\\n+                if (thing - ARENA_INFO_TO_START(a) < limit) {\\n                     root_points_to_gcArenaList = JS_TRUE;\\n                     break;\\n                 }\\n-                limit = GC_THINGS_SIZE;\\n+                limit = (size_t) THINGS_PER_ARENA(thingSize) * thingSize;\\n             }\\n         }\\n         if (!root_points_to_gcArenaList && rhe->name) {\\n@@ -2109,9 +2410,9 @@ js_GC(JSContext *cx, JSGCInvocationKind gckind)\\n     JSBool keepAtoms;\\n     uintN i, type;\\n     JSTracer trc;\\n-    size_t nbytes, limit, offset;\\n-    JSGCArena *a, **ap;\\n-    uint8 flags, *flagp, *firstPage;\\n+    uint32 thingSize, indexLimit;\\n+    JSGCArenaInfo *a, **ap;\\n+    uint8 flags, *flagp;\\n     JSGCThing *thing, *freeList;\\n     JSGCArenaList *arenaList;\\n     GCFinalizeOp finalizer;\\n@@ -2309,8 +2610,8 @@ js_GC(JSContext *cx, JSGCInvocationKind gckind)\\n \\n restart:\\n     rt->gcNumber++;\\n-    JS_ASSERT(!rt->gcUnscannedArenaStackTop);\\n-    JS_ASSERT(rt->gcUnscannedBagSize == 0);\\n+    JS_ASSERT(!rt->gcUntracedArenaStackTop);\\n+    JS_ASSERT(rt->gcTraceLaterCount == 0);\\n \\n     /*\\n      * Mark phase.\\n@@ -2325,7 +2626,7 @@ restart:\\n      * Mark children of things that caused too deep recursion during the above\\n      * tracing.\\n      */\\n-    ScanDelayedChildren(&trc);\\n+    TraceDelayedChildren(&trc);\\n \\n     JS_ASSERT(!cx->insideGCMarkCallback);\\n     if (rt->gcCallback) {\\n@@ -2334,7 +2635,7 @@ restart:\\n         JS_ASSERT(cx->insideGCMarkCallback);\\n         cx->insideGCMarkCallback = JS_FALSE;\\n     }\\n-    JS_ASSERT(rt->gcUnscannedBagSize == 0);\\n+    JS_ASSERT(rt->gcTraceLaterCount == 0);\\n \\n     rt->gcMarkingTracer = NULL;\\n \\n@@ -2382,22 +2683,18 @@ restart:\\n                                      : i == GC_FREELIST_INDEX(sizeof(JSObject))\\n                                      ? 0\\n                                      : i];\\n-        nbytes = arenaList->thingSize;\\n-        limit = arenaList->lastLimit;\\n-        for (a = arenaList->last; a; a = a->prev) {\\n-            JS_ASSERT(!a->prevUnscanned);\\n-            JS_ASSERT(a->unscannedPages == 0);\\n-            firstPage = (uint8 *) FIRST_THING_PAGE(a);\\n-            for (offset = 0; offset != limit; offset += nbytes) {\\n-                if ((offset & GC_PAGE_MASK) == 0) {\\n-                    JS_ASSERT(((JSGCPageInfo *)(firstPage + offset))->\\n-                              unscannedBitmap == 0);\\n-                    offset += PAGE_THING_GAP(nbytes);\\n-                }\\n-                JS_ASSERT(offset < limit);\\n-                flagp = a->base + offset / sizeof(JSGCThing);\\n-                if (flagp >= firstPage)\\n-                    flagp += GC_THINGS_SIZE;\\n+        a = arenaList->last;\\n+        if (!a)\\n+            continue;\\n+\\n+        thingSize = arenaList->thingSize;\\n+        indexLimit = THINGS_PER_ARENA(thingSize);\\n+        JS_ASSERT(arenaList->lastCount > 0);\\n+        flagp = THING_FLAGP(a, arenaList->lastCount - 1);\\n+        for (;;) {\\n+            JS_ASSERT(a->prevUntracedPage == 0);\\n+            JS_ASSERT(a->untracedThings == 0);\\n+            do {\\n                 flags = *flagp;\\n                 if (flags & GCF_MARK) {\\n                     *flagp &= ~GCF_MARK;\\n@@ -2406,7 +2703,7 @@ restart:\\n                     type = flags & GCF_TYPEMASK;\\n                     finalizer = gc_finalizers[type];\\n                     if (finalizer) {\\n-                        thing = (JSGCThing *)(firstPage + offset);\\n+                        thing = (JSGCThing *) FLAGP_TO_THING(flagp, thingSize);\\n                         *flagp = (uint8)(flags | GCF_FINAL);\\n                         if (type >= GCX_EXTERNAL_STRING)\\n                             js_PurgeDeflatedStringCache(rt, (JSString *)thing);\\n@@ -2416,8 +2713,11 @@ restart:\\n                     /* Set flags to GCF_FINAL, signifying that thing is free. */\\n                     *flagp = GCF_FINAL;\\n                 }\\n-            }\\n-            limit = GC_THINGS_SIZE;\\n+            } while (++flagp != THING_FLAGS_END(a));\\n+            a = a->prev;\\n+            if (!a)\\n+                break;\\n+            flagp = THING_FLAGP(a, indexLimit - 1);\\n         }\\n     }\\n \\n@@ -2442,47 +2742,45 @@ restart:\\n     for (i = 0; i < GC_NUM_FREELISTS; i++) {\\n         arenaList = &rt->gcArenaList[i];\\n         ap = &arenaList->last;\\n-        a = *ap;\\n-        if (!a)\\n+        if (!(a = *ap))\\n             continue;\\n \\n         allClear = JS_TRUE;\\n         arenaList->freeList = NULL;\\n         freeList = NULL;\\n+        thingSize = arenaList->thingSize;\\n+        indexLimit = THINGS_PER_ARENA(thingSize);\\n+        JS_ASSERT(arenaList->lastCount > 0);\\n+        flagp = THING_FLAGP(a, arenaList->lastCount - 1);\\n         METER(arenaList->stats.nthings = 0);\\n         METER(arenaList->stats.freelen = 0);\\n-\\n-        nbytes = GC_FREELIST_NBYTES(i);\\n-        limit = arenaList->lastLimit;\\n-        do {\\n+        for (;;) {\\n             METER(size_t nfree = 0);\\n-            firstPage = (uint8 *) FIRST_THING_PAGE(a);\\n-            for (offset = 0; offset != limit; offset += nbytes) {\\n-                if ((offset & GC_PAGE_MASK) == 0)\\n-                    offset += PAGE_THING_GAP(nbytes);\\n-                JS_ASSERT(offset < limit);\\n-                flagp = a->base + offset / sizeof(JSGCThing);\\n-                if (flagp >= firstPage)\\n-                    flagp += GC_THINGS_SIZE;\\n-\\n+            do {\\n                 if (*flagp != GCF_FINAL) {\\n                     allClear = JS_FALSE;\\n                     METER(++arenaList->stats.nthings);\\n                 } else {\\n-                    thing = (JSGCThing *)(firstPage + offset);\\n+                    thing = (JSGCThing *) FLAGP_TO_THING(flagp, thingSize);\\n                     thing->flagp = flagp;\\n                     thing->next = freeList;\\n                     freeList = thing;\\n                     METER(++nfree);\\n                 }\\n-            }\\n+            } while (++flagp != THING_FLAGS_END(a));\\n+\\n             if (allClear) {\\n                 /*\\n-                 * Forget just assembled free list head for the arena\\n-                 * and destroy the arena itself.\\n+                 * Forget just assembled free list head for the arena and\\n+                 * destroy the arena itself.\\n                  */\\n                 freeList = arenaList->freeList;\\n-                DestroyGCArena(rt, arenaList, ap);\\n+                if (a == arenaList->last)\\n+                    arenaList->lastCount = indexLimit;\\n+                *ap = a->prev;\\n+                JS_ASSERT(rt->gcBytes >= GC_ARENA_SIZE);\\n+                rt->gcBytes -= GC_ARENA_SIZE;\\n+                DestroyGCArena(rt, a);\\n             } else {\\n                 allClear = JS_TRUE;\\n                 arenaList->freeList = freeList;\\n@@ -2491,8 +2789,10 @@ restart:\\n                 METER(arenaList->stats.totalfreelen += nfree);\\n                 METER(++arenaList->stats.totalarenas);\\n             }\\n-            limit = GC_THINGS_SIZE;\\n-        } while ((a = *ap) != NULL);\\n+            if (!(a = *ap))\\n+                break;\\n+            flagp = THING_FLAGP(a, indexLimit - 1);\\n+        }\\n     }\\n \\n     if (rt->gcCallback)\\ndiff --git a/js/src/jsgc.h b/js/src/jsgc.h\\nindex bad22b8..a620cb2 100644\\n--- a/js/src/jsgc.h\\n+++ b/js/src/jsgc.h\\n@@ -254,10 +254,11 @@ typedef struct JSGCStats {\\n     uint32  maxdepth;   /* maximum mark tail recursion depth */\\n     uint32  cdepth;     /* mark recursion depth of C functions */\\n     uint32  maxcdepth;  /* maximum mark recursion depth of C functions */\\n-    uint32  unscanned;  /* mark C stack overflows or number of times\\n-                           GC things were put in unscanned bag */\\n+    uint32  untraced;   /* number of times tracing of GC thing's children were\\n+                           delayed due to a low C stack */\\n #ifdef DEBUG\\n-    uint32  maxunscanned;       /* maximum size of unscanned bag */\\n+    uint32  maxuntraced;/* maximum number of things with children to trace\\n+                           later */\\n #endif\\n     uint32  maxlevel;   /* maximum GC nesting (indirect recursion) level */\\n     uint32  poke;       /* number of potentially useful GC calls */\\n@@ -276,8 +277,9 @@ js_DumpGCStats(JSRuntime *rt, FILE *fp);\\n \\n #endif /* JS_GCMETER */\\n \\n-typedef struct JSGCArena JSGCArena;\\n+typedef struct JSGCArenaInfo JSGCArenaInfo;\\n typedef struct JSGCArenaList JSGCArenaList;\\n+typedef struct JSGCChunkInfo JSGCChunkInfo;\\n \\n #ifdef JS_GCMETER\\n typedef struct JSGCArenaStats JSGCArenaStats;\\n@@ -298,25 +300,26 @@ struct JSGCArenaStats {\\n #endif\\n \\n struct JSGCArenaList {\\n-    JSGCArena   *last;          /* last allocated GC arena */\\n-    uint16      lastLimit;      /* end offset of allocated so far things in\\n-                                   the last arena */\\n-    uint16      thingSize;      /* size of things to allocate on this list */\\n-    JSGCThing   *freeList;      /* list of free GC things */\\n+    JSGCArenaInfo   *last;          /* last allocated GC arena */\\n+    uint16          lastCount;      /* number of allocated things in the last\\n+                                       arena */\\n+    uint16          thingSize;      /* size of things to allocate on this list\\n+                                     */\\n+    JSGCThing       *freeList;      /* list of free GC things */\\n #ifdef JS_GCMETER\\n-    JSGCArenaStats stats;\\n+    JSGCArenaStats  stats;\\n #endif\\n };\\n \\n struct JSWeakRoots {\\n     /* Most recently created things by type, members of the GC's root set. */\\n-    JSGCThing           *newborn[GCX_NTYPES];\\n+    void            *newborn[GCX_NTYPES];\\n \\n     /* Atom root for the last-looked-up atom on this context. */\\n-    jsval               lastAtom;\\n+    jsval           lastAtom;\\n \\n     /* Root for the result of the most recent js_InternalInvoke call. */\\n-    jsval               lastInternalResult;\\n+    jsval           lastInternalResult;\\n };\\n \\n JS_STATIC_ASSERT(JSVAL_NULL == 0);\\ndiff --git a/js/src/jsinterp.c b/js/src/jsinterp.c\\nindex 8bc2d75..cff9d4b 100644\\n--- a/js/src/jsinterp.c\\n+++ b/js/src/jsinterp.c\\n@@ -5291,8 +5291,7 @@ interrupt:\\n             JS_ASSERT(sp - fp->spbase >= 1);\\n             lval = FETCH_OPND(-1);\\n             JS_ASSERT(JSVAL_IS_OBJECT(lval));\\n-            cx->weakRoots.newborn[GCX_OBJECT] =\\n-                (JSGCThing *)JSVAL_TO_GCTHING(lval);\\n+            cx->weakRoots.newborn[GCX_OBJECT] = JSVAL_TO_GCTHING(lval);\\n           END_CASE(JSOP_ENDINIT)\\n \\n           BEGIN_CASE(JSOP_INITPROP)\\ndiff --git a/js/src/jsobj.c b/js/src/jsobj.c\\nindex 6053337..3d30058 100644\\n--- a/js/src/jsobj.c\\n+++ b/js/src/jsobj.c\\n@@ -2540,7 +2540,7 @@ js_NewObject(JSContext *cx, JSClass *clasp, JSObject *proto, JSObject *parent)\\n \\n out:\\n     JS_POP_TEMP_ROOT(cx, &tvr);\\n-    cx->weakRoots.newborn[GCX_OBJECT] = (JSGCThing *) obj;\\n+    cx->weakRoots.newborn[GCX_OBJECT] = obj;\\n     return obj;\\n \\n bad:\\n@@ -4405,8 +4405,7 @@ js_GetClassPrototype(JSContext *cx, JSObject *scope, jsid id,\\n              * instance that delegates to this object, or just query the\\n              * prototype for its class.\\n              */\\n-            cx->weakRoots.newborn[GCX_OBJECT] =\\n-                (JSGCThing *)JSVAL_TO_GCTHING(v);\\n+            cx->weakRoots.newborn[GCX_OBJECT] = JSVAL_TO_GCTHING(v);\\n         }\\n     }\\n     *protop = JSVAL_IS_OBJECT(v) ? JSVAL_TO_OBJECT(v) : NULL;\\n\""}